{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh2QjWPgDNzJ"
      },
      "source": [
        "##Data Prep##\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sb3C9dnYc7aP"
      },
      "outputs": [],
      "source": [
        "import gdown\n",
        "\n",
        "# Folder ID from the URL\n",
        "folder_id = '1F3bTRe7dZe2gxPom3LzYPzkfyNSk-8VJ'\n",
        "\n",
        "# Using gdown to download the folder contents\n",
        "url = f'https://drive.google.com/drive/folders/1F3bTRe7dZe2gxPom3LzYPzkfyNSk-8VJ'\n",
        "gdown.download_folder(url, quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hQ5GobYXdJb2"
      },
      "outputs": [],
      "source": [
        "#Urdu dataset uploading\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "urdu_zip_path = '/content/NLP Project/Urdu Fake News Dataset.zip'\n",
        "\n",
        "with zipfile.ZipFile(urdu_zip_path, 'r') as zip_ref:\n",
        "\n",
        "  zip_ref.extractall('/content/Urdu')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSrba9a3fdvr"
      },
      "outputs": [],
      "source": [
        "#Urdu dataframe loading\n",
        "def load_news_data(directory, label):\n",
        "    news_data = []\n",
        "    for filename in os.listdir(directory):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            content = file.read().strip()\n",
        "            news_data.append({'text': content, 'label': label})\n",
        "    return news_data\n",
        "\n",
        "urdu_trainfake_path = '/content/Urdu/1.Corpus/Train/Fake'\n",
        "urdu_trainreal_path = '/content/Urdu/1.Corpus/Train/Real'\n",
        "urdu_testfake_path = '/content/Urdu/1.Corpus/Test/Fake'\n",
        "urdu_testreal_path = '/content/Urdu/1.Corpus/Test/Real'\n",
        "\n",
        "urdu_trainfake_texts = load_news_data(urdu_trainfake_path, 'Fake')\n",
        "urdu_trainreal_texts = load_news_data(urdu_trainreal_path, 'True')\n",
        "urdu_testfake_texts = load_news_data(urdu_testfake_path, 'Fake')\n",
        "urdu_testreal_texts = load_news_data(urdu_testreal_path, 'True')\n",
        "\n",
        "# Creating a DataFrame for the Urdu dataset\n",
        "urdu_news_df = pd.DataFrame(urdu_trainfake_texts+urdu_trainreal_texts+urdu_testfake_texts+urdu_testreal_texts)\n",
        "urdu_news_df['source'] = 'Urdu'\n",
        "\n",
        "urdu_news_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktKFllZdpafy"
      },
      "outputs": [],
      "source": [
        "urdu_news_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-p8OaIpnobwg"
      },
      "outputs": [],
      "source": [
        "#Loading the english dataset\n",
        "english_excel_path = '/content/NLP Project/pakistani_dataset_consolidated (1).xlsx'\n",
        "english_dataset = pd.read_excel(english_excel_path)\n",
        "\n",
        "english_dataset = english_dataset.dropna(subset=['Text'])\n",
        "\n",
        "if 'cleaned_labels' in english_dataset.columns:\n",
        "    english_dataset = english_dataset.rename(columns={'Text': 'text', 'cleaned_labels': 'label'})\n",
        "elif 'Textual Rating' in english_dataset.columns:  # If another column exists for labels\n",
        "    english_dataset = english_dataset.rename(columns={'Text': 'text', 'Textual Rating': 'label'})\n",
        "else:\n",
        "    raise KeyError(\"No valid label column found in the English dataset.\")\n",
        "\n",
        "#Adding the source column for tracking the dataset origin\n",
        "english_dataset['source'] = 'English'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JJ4tXv71rbuA"
      },
      "outputs": [],
      "source": [
        "english_dataset.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d2ubseVGFpU6"
      },
      "outputs": [],
      "source": [
        "#Combining the two datasets\n",
        "combined_dataset = pd.concat([english_dataset[['text', 'label', 'source']], urdu_news_df], ignore_index=True)\n",
        "\n",
        "# Handling any remaining missing values in the combined dataset\n",
        "combined_dataset = combined_dataset.dropna(subset=['text'])\n",
        "\n",
        "# Mapping labels to just 'True', 'False', 'Unclear'\n",
        "def simplify_labels(label):\n",
        "    label = str(label)\n",
        "    label = label.lower()\n",
        "    if 'true' in label or 'partly true' in label or 'half true' in label:\n",
        "        return 'True'\n",
        "    elif 'false' in label or 'fake' in label or 'hoax' in label or 'doctored' in label:\n",
        "        return 'False'\n",
        "    else:\n",
        "        return 'Unclear'\n",
        "\n",
        "combined_dataset['label'] = combined_dataset['label'].apply(simplify_labels)\n",
        "\n",
        "#Separating English and Urdu data for vectorization\n",
        "english_data = combined_dataset[combined_dataset['source'] == 'English']\n",
        "urdu_data = combined_dataset[combined_dataset['source'] == 'Urdu']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIPy-7EMAlfQ"
      },
      "outputs": [],
      "source": [
        "combined_dataset = combined_dataset.loc[combined_dataset['label'] != 'Unclear']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_E_ASTiBg_V"
      },
      "outputs": [],
      "source": [
        "unique_labels = combined_dataset['label'].unique()\n",
        "\n",
        "combined_dataset['label'] = combined_dataset['label'].map({'False': 0, 'True': 1})\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fZehts-NCBLh",
        "outputId": "31bf15db-e377-4ac6-f9a5-96c4b76e91fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "int64\n"
          ]
        }
      ],
      "source": [
        "print(combined_dataset['label'].dtypes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3AYplU5DWHG"
      },
      "source": [
        "##Fake News Bert##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vNlcsGy9Dfdz"
      },
      "outputs": [],
      "source": [
        "from transformers import  AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"jy46604790/Fake-News-Bert-Detect\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdq6ydCaGf73"
      },
      "outputs": [],
      "source": [
        "# Ensure texts column is available and cleaned\n",
        "if hasattr(combined_dataset['text'], 'tolist'):\n",
        "    texts = combined_dataset['text'].tolist()\n",
        "else:\n",
        "    texts = combined_dataset['text']\n",
        "\n",
        "# Apply tokenizer to each entry and store tokenized data in a new column\n",
        "combined_dataset['tokenized'] = combined_dataset['text'].apply(\n",
        "    lambda x: tokenizer(\n",
        "        x,\n",
        "        padding='max_length',  # Ensure all sequences are padded to the same length\n",
        "        truncation=True        # Truncate longer sequences to max length\n",
        "    )\n",
        ")\n",
        "\n",
        "# If you only want specific parts (like input_ids), you can do:\n",
        "combined_dataset['input_ids'] = combined_dataset['text'].apply(\n",
        "    lambda x: tokenizer(x, padding='max_length', truncation=True)['input_ids']\n",
        ")\n",
        "\n",
        "# # Print the modified dataset\n",
        "# print(combined_dataset.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkZs_KDnFzUF"
      },
      "outputs": [],
      "source": [
        "tokenized = combined_dataset['tokenized']\n",
        "labels = combined_dataset['label']\n",
        "print(len(tokenized))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1OpLC5Bwzf_"
      },
      "outputs": [],
      "source": [
        "combined_dataset.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zS8kfStnHJFd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Extract input_ids and attention_mask\n",
        "input_ids = torch.tensor(combined_dataset['input_ids'].tolist())\n",
        "attention_mask = torch.tensor(\n",
        "    [entry['attention_mask'] for entry in combined_dataset['tokenized']]\n",
        ")\n",
        "\n",
        "# Convert labels to tensor\n",
        "labels = torch.tensor(combined_dataset['label'].tolist())\n",
        "\n",
        "print(input_ids.shape, attention_mask.shape, labels.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X55RCPwtHM89"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = input_ids\n",
        "y = labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optionally, split attention masks if needed\n",
        "attention_mask_train, attention_mask_test = train_test_split(attention_mask, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCH1b290HTF1"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Combine tensors into a dataset\n",
        "train_dataset = TensorDataset(X_train, attention_mask_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, attention_mask_test, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VGi7BdwmHZUr"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "cYVdCl7_HdDT"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UXVC8me6S5T-"
      },
      "outputs": [],
      "source": [
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        outputs = model(input_ids, attention_mask=attention_mask)\n",
        "        predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "        correct += (predictions == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Store the predictions and true labels for classification report\n",
        "        all_preds.extend(predictions.cpu().numpy())\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePj18sfpC2Ww"
      },
      "source": [
        "##Roberta##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ELYAa0KC5R7"
      },
      "outputs": [],
      "source": [
        "from transformers import  AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"Pavan48/fake_news_detection_roberta\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Pavan48/fake_news_detection_roberta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O6hQlFmcDXZM"
      },
      "outputs": [],
      "source": [
        "# Ensure texts column is available and cleaned\n",
        "if hasattr(combined_dataset['text'], 'tolist'):\n",
        "    texts = combined_dataset['text'].tolist()\n",
        "else:\n",
        "    texts = combined_dataset['text']\n",
        "\n",
        "# Apply tokenizer to each entry and store tokenized data in a new column\n",
        "combined_dataset['tokenized'] = combined_dataset['text'].apply(\n",
        "    lambda x: tokenizer(\n",
        "        x,\n",
        "        padding='max_length',  # Ensure all sequences are padded to the same length\n",
        "        truncation=True        # Truncate longer sequences to max length\n",
        "    )\n",
        ")\n",
        "\n",
        "# If you only want specific parts (like input_ids), you can do:\n",
        "combined_dataset['input_ids'] = combined_dataset['text'].apply(\n",
        "    lambda x: tokenizer(x, padding='max_length', truncation=True)['input_ids']\n",
        ")\n",
        "\n",
        "# # Print the modified dataset\n",
        "# print(combined_dataset.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Guj5vPemDhu8"
      },
      "outputs": [],
      "source": [
        "tokenized = combined_dataset['tokenized']\n",
        "labels = combined_dataset['label']\n",
        "print(len(tokenized))\n",
        "print(len(labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ky3ZkKI8Dnay"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Extract input_ids and attention_mask\n",
        "input_ids = torch.tensor(combined_dataset['input_ids'].tolist())\n",
        "attention_mask = torch.tensor(\n",
        "    [entry['attention_mask'] for entry in combined_dataset['tokenized']]\n",
        ")\n",
        "\n",
        "# Convert labels to tensor\n",
        "labels = torch.tensor(combined_dataset['label'].tolist())\n",
        "\n",
        "print(input_ids.shape, attention_mask.shape, labels.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FE-KT_jTDt1Q"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = input_ids\n",
        "y = labels\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Optionally, split attention masks if needed\n",
        "attention_mask_train, attention_mask_test = train_test_split(attention_mask, test_size=0.2, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FF5fezlADxiC"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Combine tensors into a dataset\n",
        "train_dataset = TensorDataset(X_train, attention_mask_train, y_train)\n",
        "test_dataset = TensorDataset(X_test, attention_mask_test, y_test)\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wxBYBsdNyE3r"
      },
      "outputs": [],
      "source": [
        "len(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YgZOyiiJD1D9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Move to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKn3mgP9D43d"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "epochs = 3\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader):\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yX1q0Mc1PokA"
      },
      "outputs": [],
      "source": [
        "len(y_test)\n",
        "print(total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZOSf12wUx1gy"
      },
      "outputs": [],
      "source": [
        "len(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D8eoHy_yrqz"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "\n",
        "        # Loop over each sample in the batch\n",
        "        for i in range(input_ids.size(0)):\n",
        "            input_id = input_ids[i].unsqueeze(0)  # Select the i-th input in the batch\n",
        "            mask = attention_mask[i].unsqueeze(0)  # Select the i-th attention mask\n",
        "            label = labels[i].unsqueeze(0)  # Select the i-th label\n",
        "\n",
        "            # Get model predictions for the individual sample\n",
        "            outputs = model(input_id, attention_mask=mask)\n",
        "            prediction = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            # Update correct and total counters\n",
        "            correct += (prediction == label).sum().item()\n",
        "            total += label.size(0)\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WcHThTjrzKPK"
      },
      "outputs": [],
      "source": [
        "print(total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HP_PxvQ3PcpG"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Convert tensors to CPU and NumPy arrays\n",
        "preds = predictions.cpu().numpy()\n",
        "true_labels = labels.cpu().numpy()\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, preds)\n",
        "\n",
        "# Plot confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5tX3lJRt0VJA"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "report = classification_report(all_labels, all_predictions, output_dict=True)\n",
        "print(report)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XLM-RoBERTa ##\n"
      ],
      "metadata": {
        "id": "1Dx_FH8mxshL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "# Function to tokenize the text data\n",
        "def tokenize_data(texts, tokenizer, max_length=512):\n",
        "    return tokenizer(texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splitting the data (80% for training and 20% for testing)\n",
        "train_data, test_data = train_test_split(combined_dataset, test_size=0.2, random_state=42)\n",
        "\n",
        "# Print the shape to confirm the split\n",
        "print(\"Training data shape:\", train_data.shape)\n",
        "print(\"Testing data shape:\", test_data.shape)\n"
      ],
      "metadata": {
        "id": "V0nLrUA9xw3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "# Define a custom dataset class\n",
        "class FakeNewsDataset(Dataset):\n",
        "    def __init__(self, data, tokenizer, max_len):\n",
        "        self.data = data\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.data.iloc[idx]['text']\n",
        "        label = self.data.iloc[idx]['label']\n",
        "\n",
        "        encoding = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "# Initialize the tokenizer\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
        "\n",
        "# Set max sequence length for padding/truncating\n",
        "max_len = 512\n",
        "\n",
        "# Create datasets and dataloaders\n",
        "train_dataset = FakeNewsDataset(train_data, tokenizer, max_len)\n",
        "test_dataset = FakeNewsDataset(test_data, tokenizer, max_len)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
      ],
      "metadata": {
        "id": "jOKe1boAqeNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaForSequenceClassification\n",
        "\n",
        "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-roberta-base', num_labels=2)\n"
      ],
      "metadata": {
        "id": "P2Aq7bjTqo0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from transformers import AdamW\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import torch\n",
        "\n",
        "# Define optimizer and scheduler\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "scheduler = StepLR(optimizer, step_size=3, gamma=0.1)\n",
        "\n",
        "# Set device for GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Training Loop\n",
        "epochs = 3\n",
        "losses = []  # Store losses for plotting\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for batch in dataloader:\n",
        "        input_ids = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "    # Save the average loss for this epoch\n",
        "    avg_loss = total_loss / len(dataloader)\n",
        "    losses.append(avg_loss)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss}\")\n",
        "\n",
        "# Plotting the loss graph\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(range(1, epochs+1), losses, marker='o', linestyle='-', color='b', label='Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "aGi18Pf2qpkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "predictions, true_labels = [], []\n",
        "# Function to evaluate on the test set\n",
        "def evaluate_on_test_set(model, test_dataloader):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in test_dataloader:  # Use test_dataloader here\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits\n",
        "            preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "            predictions.extend(preds)\n",
        "            true_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Print the classification report for the test set\n",
        "    print(\"Classification Report (Test Set):\")\n",
        "    print(classification_report(true_labels, predictions, target_names=['False', 'True']))\n",
        "\n",
        "    # Generate confusion matrix for the test set\n",
        "    cm = confusion_matrix(true_labels, predictions)\n",
        "\n",
        "    # Display the confusion matrix for the test set\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['False', 'True'])\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix (Test Set)\")\n",
        "    plt.show()\n",
        "\n",
        "# Assuming your model is already trained, call the evaluate function\n",
        "evaluate_on_test_set(model, test_dataloader)\n"
      ],
      "metadata": {
        "id": "RQnjYECJqr-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNKn5XpbDa4N"
      },
      "source": [
        "##Baseline##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kkx03aMPotl4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from scipy.sparse import vstack\n",
        "import zipfile\n",
        "\n",
        "#Combining the two datasets\n",
        "combined_dataset = pd.concat([english_dataset[['text', 'label', 'source']], urdu_news_df], ignore_index=True)\n",
        "\n",
        "# Handling any remaining missing values in the combined dataset\n",
        "combined_dataset = combined_dataset.dropna(subset=['text'])\n",
        "\n",
        "# Mapping labels to just 'True', 'False', 'Unclear'\n",
        "def simplify_labels(label):\n",
        "    label = str(label)\n",
        "    label = label.lower()\n",
        "    if 'true' in label or 'partly true' in label or 'half true' in label:\n",
        "        return 'True'\n",
        "    elif 'false' in label or 'fake' in label or 'hoax' in label or 'doctored' in label:\n",
        "        return 'False'\n",
        "    else:\n",
        "        return 'Unclear'\n",
        "\n",
        "combined_dataset['label'] = combined_dataset['label'].apply(simplify_labels)\n",
        "\n",
        "#Separating English and Urdu data for vectorization\n",
        "english_data = combined_dataset[combined_dataset['source'] == 'English']\n",
        "urdu_data = combined_dataset[combined_dataset['source'] == 'Urdu']\n",
        "\n",
        "# Vectorizing the English text using TF-IDF\n",
        "print(\"Vectorizing English data...\")\n",
        "tfidf_vectorizer_en = TfidfVectorizer(max_features=5000)\n",
        "X_english_tfidf = tfidf_vectorizer_en.fit_transform(english_data['text'])\n",
        "\n",
        "# Vectorizing the Urdu text using TF-IDF\n",
        "print(\"Vectorizing Urdu data...\")\n",
        "tfidf_vectorizer_ur = TfidfVectorizer(max_features=5000, token_pattern=r'\\w+')\n",
        "X_urdu_tfidf = tfidf_vectorizer_ur.fit_transform(urdu_data['text'])\n",
        "\n",
        "# Combining the vectorized data\n",
        "X_combined_tfidf = vstack([X_english_tfidf, X_urdu_tfidf])\n",
        "\n",
        "# Combining the labels\n",
        "y_combined = pd.concat([english_data['label'], urdu_data['label']], ignore_index=True)\n",
        "\n",
        "# Training the Logistic Regression model\n",
        "print(\"Training the Logistic Regression model...\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_combined_tfidf, y_combined, test_size=0.2, random_state=42)\n",
        "lr_combined_model = LogisticRegression(max_iter=1000)\n",
        "lr_combined_model.fit(X_train, y_train)\n",
        "\n",
        "# Predicting and evaluating\n",
        "print(\"Making predictions and evaluating the model...\")\n",
        "y_pred_combined = lr_combined_model.predict(X_test)\n",
        "\n",
        "# Printing Report\n",
        "classification_report_combined = classification_report(y_test, y_pred_combined)\n",
        "print(\"\\nClassification Report:\\n\", classification_report_combined)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "sh2QjWPgDNzJ",
        "ePj18sfpC2Ww",
        "gNKn5XpbDa4N"
      ],
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}